import cv2
import numpy as np
from ultralytics import YOLO
from collections import deque
import time
import threading
from utils import config


class MultiObjectTracker:
    """å¤šç›®æ ‡è·Ÿè¸ªå™¨ - ç”¨äºå¤„ç†å¤šä¸ªç¾½æ¯›çƒ"""
    
    def __init__(self, max_objects=2, distance_threshold=100):
        self.max_objects = max_objects
        self.distance_threshold = distance_threshold
        self.tracks = {}  # track_id -> track_info
        self.next_track_id = 0
        self.max_missing_frames = 10
        
        print(f"ğŸ¯ Multi-object tracker initialized (max_objects={max_objects})")
    
    def update(self, detections, timestamp):
        """æ›´æ–°è·Ÿè¸ªå™¨"""
        # æ ‡è®°æ‰€æœ‰ç°æœ‰è½¨è¿¹ä¸ºä¸¢å¤±
        for track_id in self.tracks:
            self.tracks[track_id]['missing_frames'] += 1
        
        # å°†æ£€æµ‹ç»“æœåˆ†é…ç»™ç°æœ‰è½¨è¿¹æˆ–åˆ›å»ºæ–°è½¨è¿¹
        for detection in detections:
            pos, conf = detection
            best_track_id = self._find_best_match(pos)
            
            if best_track_id is not None:
                # æ›´æ–°ç°æœ‰è½¨è¿¹
                self._update_track(best_track_id, pos, conf, timestamp)
            elif len(self.tracks) < self.max_objects:
                # åˆ›å»ºæ–°è½¨è¿¹
                self._create_track(pos, conf, timestamp)
        
        # ç§»é™¤é•¿æ—¶é—´ä¸¢å¤±çš„è½¨è¿¹
        self._remove_lost_tracks()
        
        return list(self.tracks.keys())
    
    def _find_best_match(self, pos):
        """æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„è½¨è¿¹"""
        best_track_id = None
        min_distance = float('inf')
        
        for track_id, track in self.tracks.items():
            if len(track['positions']) > 0:
                last_pos = track['positions'][-1]
                distance = np.linalg.norm(np.array(pos) - np.array(last_pos))
                
                if distance < self.distance_threshold and distance < min_distance:
                    min_distance = distance
                    best_track_id = track_id
        
        return best_track_id
    
    def _create_track(self, pos, conf, timestamp):
        """åˆ›å»ºæ–°è½¨è¿¹"""
        track_id = self.next_track_id
        self.next_track_id += 1
        
        self.tracks[track_id] = {
            'positions': [pos],
            'confidences': [conf],
            'timestamps': [timestamp],
            'missing_frames': 0,
            'created_at': timestamp
        }
        
        print(f"ğŸ†• Created new track {track_id} at position {pos}")
    
    def _update_track(self, track_id, pos, conf, timestamp):
        """æ›´æ–°è½¨è¿¹"""
        track = self.tracks[track_id]
        track['positions'].append(pos)
        track['confidences'].append(conf)
        track['timestamps'].append(timestamp)
        track['missing_frames'] = 0
        
        # é™åˆ¶è½¨è¿¹é•¿åº¦
        max_history = 50
        if len(track['positions']) > max_history:
            track['positions'] = track['positions'][-max_history:]
            track['confidences'] = track['confidences'][-max_history:]
            track['timestamps'] = track['timestamps'][-max_history:]
    
    def _remove_lost_tracks(self):
        """ç§»é™¤ä¸¢å¤±çš„è½¨è¿¹"""
        to_remove = []
        for track_id, track in self.tracks.items():
            if track['missing_frames'] > self.max_missing_frames:
                to_remove.append(track_id)
        
        for track_id in to_remove:
            print(f"ğŸ—‘ï¸ Removing lost track {track_id}")
            del self.tracks[track_id]
    
    def get_tracks(self):
        """è·å–æ‰€æœ‰æ´»è·ƒè½¨è¿¹"""
        return self.tracks
    
    def get_best_track(self):
        """è·å–æœ€ä½³è½¨è¿¹ï¼ˆæœ€é•¿ä¸”æœ€è¿‘æ´»è·ƒï¼‰"""
        if not self.tracks:
            return None, None
        
        best_track_id = None
        best_score = -1
        
        for track_id, track in self.tracks.items():
            # è¯„åˆ†æ ‡å‡†ï¼šè½¨è¿¹é•¿åº¦ + æœ€è¿‘æ´»è·ƒç¨‹åº¦ + å¹³å‡ç½®ä¿¡åº¦
            length_score = len(track['positions'])
            recency_score = max(0, 10 - track['missing_frames'])
            confidence_score = np.mean(track['confidences']) if track['confidences'] else 0
            
            total_score = length_score + recency_score + confidence_score * 10
            
            if total_score > best_score:
                best_score = total_score
                best_track_id = track_id
        
        if best_track_id is not None:
            return best_track_id, self.tracks[best_track_id]
        return None, None


class TrajectoryQualityEvaluator:
    """è½¨è¿¹è´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        # è¯„ä¼°æƒé‡
        self.physics_weight = 0.3
        self.continuity_weight = 0.25
        self.completeness_weight = 0.25
        self.temporal_weight = 0.2

        # ç‰©ç†æ¨¡å‹å‚æ•°
        self.gravity = 9.8 * 100  # cm/sÂ²
        self.min_trajectory_length = 8  # æœ€å°‘è½¨è¿¹ç‚¹æ•°
        self.max_speed_change = 1000  # æœ€å¤§é€Ÿåº¦å˜åŒ– cm/s

    def evaluate_trajectory_segment(self, points_3d, timestamps, current_time):
        """è¯„ä¼°è½¨è¿¹ç‰‡æ®µè´¨é‡"""
        if len(points_3d) < 3:
            return 0.0

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        points = np.array(points_3d)
        times = np.array(timestamps)

        # è®¡ç®—å„é¡¹å¾—åˆ†
        physics_score = self._evaluate_physics(points, times)
        continuity_score = self._evaluate_continuity(points, times)
        completeness_score = self._evaluate_completeness(points, times)
        temporal_score = self._evaluate_temporal_weight(times, current_time)

        # ç»¼åˆå¾—åˆ†
        total_score = (
                physics_score * self.physics_weight +
                continuity_score * self.continuity_weight +
                completeness_score * self.completeness_weight +
                temporal_score * self.temporal_weight
        )

        return total_score

    def _evaluate_physics(self, points, times):
        """è¯„ä¼°ç‰©ç†åˆç†æ€§"""
        if len(points) < 3:
            return 0.0

        try:
            # è®¡ç®—é€Ÿåº¦
            velocities = []
            for i in range(1, len(points)):
                dt = times[i] - times[i - 1]
                if dt > 0:
                    vel = (points[i] - points[i - 1]) / dt
                    velocities.append(vel)

            if len(velocities) < 2:
                return 0.0

            velocities = np.array(velocities)

            # 1. æ£€æŸ¥Zæ–¹å‘æ˜¯å¦æœ‰ä¸‹é™è¶‹åŠ¿
            z_velocities = velocities[:, 2]
            has_downward_trend = np.mean(z_velocities) < -50  # å¹³å‡å‘ä¸‹é€Ÿåº¦ > 50 cm/s

            # 2. æ£€æŸ¥é€Ÿåº¦å˜åŒ–çš„å¹³æ»‘æ€§
            speed_changes = []
            for i in range(1, len(velocities)):
                speed_change = np.linalg.norm(velocities[i] - velocities[i - 1])
                speed_changes.append(speed_change)

            avg_speed_change = np.mean(speed_changes) if speed_changes else 0
            smooth_score = max(0, 1 - avg_speed_change / self.max_speed_change)

            # 3. æ£€æŸ¥è½¨è¿¹å½¢çŠ¶æ˜¯å¦æ¥è¿‘æŠ›ç‰©çº¿
            # ç®€åŒ–æ£€æŸ¥ï¼šZåæ ‡éšæ—¶é—´çš„äºŒæ¬¡æ‹Ÿåˆ
            try:
                z_coords = points[:, 2]
                time_relative = times - times[0]
                poly_coeffs = np.polyfit(time_relative, z_coords, 2)
                poly_fit = np.polyval(poly_coeffs, time_relative)
                fit_error = np.mean(np.abs(z_coords - poly_fit))
                parabola_score = max(0, 1 - fit_error / 100)  # è¯¯å·®å°äº10cmå¾—æ»¡åˆ†
            except:
                parabola_score = 0.5

            # ç»¼åˆç‰©ç†å¾—åˆ†
            physics_score = (
                    (1.0 if has_downward_trend else 0.3) * 0.4 +
                    smooth_score * 0.3 +
                    parabola_score * 0.3
            )

            return min(1.0, physics_score)

        except Exception as e:
            return 0.0

    def _evaluate_continuity(self, points, times):
        """è¯„ä¼°è¿ç»­æ€§"""
        if len(points) < 2:
            return 0.0

        # æ—¶é—´é—´éš”çš„ä¸€è‡´æ€§
        time_intervals = np.diff(times)
        expected_interval = 1.0 / 30.0  # 30fps

        # è®¡ç®—æ—¶é—´é—´éš”çš„æ ‡å‡†å·®
        interval_std = np.std(time_intervals)
        time_consistency = max(0, 1 - interval_std / expected_interval)

        # ç©ºé—´è·ç¦»çš„åˆç†æ€§
        distances = []
        for i in range(1, len(points)):
            dist = np.linalg.norm(points[i] - points[i - 1])
            distances.append(dist)

        avg_distance = np.mean(distances) if distances else 0
        # åˆç†çš„å¸§é—´è·ç¦»åº”è¯¥åœ¨1-50cmä¹‹é—´
        distance_score = 1.0 if 1 <= avg_distance <= 50 else max(0, 1 - abs(avg_distance - 25) / 100)

        # ç»¼åˆè¿ç»­æ€§å¾—åˆ†
        continuity_score = (time_consistency * 0.6 + distance_score * 0.4)

        return continuity_score

    def _evaluate_completeness(self, points, times):
        """è¯„ä¼°å®Œæ•´æ€§"""
        if len(points) < self.min_trajectory_length:
            length_score = len(points) / self.min_trajectory_length
        else:
            length_score = 1.0

        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶³å¤Ÿçš„ä¸‹é™æ®µ
        z_coords = points[:, 2]
        z_range = np.max(z_coords) - np.min(z_coords)
        height_score = min(1.0, z_range / 100)  # 1ç±³é«˜åº¦å·®å¾—æ»¡åˆ†

        # æ£€æŸ¥æ˜¯å¦æ¥è¿‘åœ°é¢
        min_height = np.min(z_coords)
        ground_proximity = max(0, 1 - min_height / 200)  # 2ç±³ä»¥ä¸‹å¼€å§‹å¾—åˆ†

        # æ—¶é—´è·¨åº¦
        time_span = times[-1] - times[0]
        time_score = min(1.0, time_span / 1.0)  # 1ç§’æ—¶é—´è·¨åº¦å¾—æ»¡åˆ†

        completeness_score = (
                length_score * 0.3 +
                height_score * 0.3 +
                ground_proximity * 0.2 +
                time_score * 0.2
        )

        return completeness_score

    def _evaluate_temporal_weight(self, times, current_time):
        """è¯„ä¼°æ—¶é—´æƒé‡ï¼ˆè¶Šè¿‘çš„è½¨è¿¹æƒé‡è¶Šé«˜ï¼‰"""
        if len(times) == 0:
            return 0.0

        latest_time = times[-1]
        time_diff = current_time - latest_time

        # æŒ‡æ•°è¡°å‡ï¼Œ2ç§’å†…ä¿æŒé«˜æƒé‡
        temporal_score = np.exp(-time_diff / 2.0)

        return temporal_score


class TrajectorySegmentManager:
    """è½¨è¿¹ç‰‡æ®µç®¡ç†å™¨"""

    def __init__(self):
        self.quality_evaluator = TrajectoryQualityEvaluator()
        self.min_segment_length = 5
        self.segment_overlap = 0.3  # ç‰‡æ®µé‡å 30%

    def find_best_trajectory_segment(self, points_3d, timestamps, current_time):
        """æ‰¾åˆ°æœ€ä½³è½¨è¿¹ç‰‡æ®µ"""
        if len(points_3d) < self.min_segment_length:
            return None, None, 0.0

        best_segment = None
        best_timestamps = None
        best_score = 0.0

        # æ»‘åŠ¨çª—å£è¯„ä¼°ä¸åŒç‰‡æ®µ
        segment_length = max(self.min_segment_length, len(points_3d) // 3)
        step_size = max(1, int(segment_length * (1 - self.segment_overlap)))

        for start_idx in range(0, len(points_3d) - self.min_segment_length + 1, step_size):
            end_idx = min(start_idx + segment_length, len(points_3d))

            segment_points = points_3d[start_idx:end_idx]
            segment_times = timestamps[start_idx:end_idx]

            score = self.quality_evaluator.evaluate_trajectory_segment(
                segment_points, segment_times, current_time
            )

            if score > best_score:
                best_score = score
                best_segment = segment_points
                best_timestamps = segment_times

        return best_segment, best_timestamps, best_score


class BufferedImageProcessor:
    """ç¼“å†²å›¾åƒå¤„ç†å™¨"""

    def __init__(self, model_path, buffer_duration=5.0, fps=30):
        self.model = YOLO(model_path)
        self.buffer_duration = buffer_duration
        self.fps = fps
        self.max_buffer_size = int(buffer_duration * fps)

        # å›¾åƒç¼“å†²åŒº
        self.image_buffer1 = deque(maxlen=self.max_buffer_size)
        self.image_buffer2 = deque(maxlen=self.max_buffer_size)
        self.timestamp_buffer = deque(maxlen=self.max_buffer_size)
        # Enhanced state management
        self.processing_lock = threading.Lock()  # Add thread safety
        self.last_processing_time = 0
        self.min_processing_interval = 2.0  # Minimum 2 seconds between processing

        # å¤„ç†çŠ¶æ€
        self.is_processing = False
        self.processing_thread = None
        self.processing_callback = None

        print(f"BufferedImageProcessor initialized with {buffer_duration}s buffer")

    def add_frame_pair(self, frame1, frame2, frame_index):
        """ä½¿ç”¨åŸºäºå¸§å·çš„æ—¶é—´æˆ³ï¼ˆé€‚ç”¨äºæœ¬åœ°è§†é¢‘ï¼‰"""
        timestamp = frame_index / self.fps  # å…³é”®ä¿®æ”¹ï¼šæŒ‰è§†é¢‘å¸§ç‡ç”Ÿæˆæ—¶é—´æˆ³
        self.image_buffer1.append(frame1.copy() if frame1 is not None else None)
        self.image_buffer2.append(frame2.copy() if frame2 is not None else None)
        self.timestamp_buffer.append(timestamp)

    def trigger_processing(self, callback=None):
        """Thread-safe processing trigger with cooldown"""
        current_time = time.time()

        with self.processing_lock:
            if self.is_processing:
                print("âš ï¸ Processing already in progress...")
                return False

            # Check cooldown period
            if current_time - self.last_processing_time < self.min_processing_interval:
                remaining = self.min_processing_interval - (current_time - self.last_processing_time)
                print(f"â±ï¸ Processing cooldown: {remaining:.1f}s remaining")
                return False

            if len(self.image_buffer1) < 10:
                print("âŒ Insufficient buffered frames for processing")
                return False

            self.processing_callback = callback
            self.is_processing = True
            self.last_processing_time = current_time

        # Start processing thread
        self.processing_thread = threading.Thread(target=self._process_buffered_frames)
        self.processing_thread.daemon = True
        self.processing_thread.start()

        return True



    def _process_buffered_frames(self):
        """å¤„ç†ç¼“å†²çš„å¸§"""
        try:
            print(f"Processing {len(self.image_buffer1)} buffered frames...")

            # å¤åˆ¶ç¼“å†²åŒºæ•°æ®ä»¥é¿å…å¤„ç†æœŸé—´çš„ä¿®æ”¹
            frames1 = list(self.image_buffer1)
            frames2 = list(self.image_buffer2)
            timestamps = list(self.timestamp_buffer)

            # æ‰¹é‡YOLOæ£€æµ‹
            all_detections1 = []
            all_detections2 = []

            # å¤„ç†ç›¸æœº1
            for frame in frames1:
                if frame is not None:
                    detections = self._detect_shuttlecock_in_frame(frame)
                else:
                    detections = []
                all_detections1.append(detections)

            # å¤„ç†ç›¸æœº2
            for frame in frames2:
                if frame is not None:
                    detections = self._detect_shuttlecock_in_frame(frame)
                else:
                    detections = []
                all_detections2.append(detections)

            # å›è°ƒå¤„ç†ç»“æœ
            if self.processing_callback:
                self.processing_callback(
                    all_detections1, all_detections2, timestamps,
                    frames1, frames2
                )

        except Exception as e:
            print(f"âŒ Error in processing buffered frames: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Ensure state is always reset
            with self.processing_lock:
                self.is_processing = False
            print("âœ… Buffered frame processing completed")

    def force_reset_processing_state(self):
        """Force reset processing state (for emergency cleanup)"""
        with self.processing_lock:
            self.is_processing = False
            self.processing_callback = None
            if self.processing_thread and self.processing_thread.is_alive():
                # Don't join, just mark as completed
                pass
        print("ğŸ”„ Processing state force reset")

    def clear_buffer(self):
        """Enhanced buffer clearing with state reset"""
        with self.processing_lock:
            if not self.is_processing:
                self.image_buffer1.clear()
                self.image_buffer2.clear()
                self.timestamp_buffer.clear()
                print("âœ… Image buffer cleared")
            else:
                print("âš ï¸ Cannot clear buffer while processing")

    def _detect_shuttlecock_in_frame(self, frame):
        """åœ¨å•å¸§ä¸­æ£€æµ‹ç¾½æ¯›çƒ - æ”¯æŒå¤šä¸ªç¾½æ¯›çƒ"""
        if frame is None:
            return []

        results = self.model(frame, conf=0.3, verbose=False)
        detections = []

        for r in results:
            # å¤„ç†å…³é”®ç‚¹ç»“æœ
            if hasattr(r, 'keypoints') and r.keypoints is not None:
                kpts = r.keypoints.xy.cpu().numpy() if hasattr(r.keypoints.xy, "cpu") else r.keypoints.xy
                for kp_list in kpts:
                    for kp in kp_list:
                        if not np.isnan(kp).any():
                            pos = (int(kp[0]), int(kp[1]))
                            detections.append((pos, 1.0))

            # å¤„ç†è¾¹ç•Œæ¡†ç»“æœ
            if hasattr(r, 'boxes') and len(r.boxes) > 0:
                boxes = r.boxes.xyxy.cpu().numpy() if hasattr(r.boxes.xyxy, "cpu") else r.boxes.xyxy
                classes = r.boxes.cls.cpu().numpy() if hasattr(r.boxes.cls, "cpu") else r.boxes.cls
                confidences = r.boxes.conf.cpu().numpy() if hasattr(r.boxes.conf, "cpu") else r.boxes.conf

                shuttlecock_indices = np.where(classes == 0)[0]
                for idx in shuttlecock_indices:
                    box = boxes[idx]
                    conf = confidences[idx]
                    x1, y1, x2, y2 = map(int, box)
                    center = ((x1 + x2) // 2, (y1 + y2) // 2)
                    detections.append((center, conf))

        # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªç¾½æ¯›çƒï¼ŒæŒ‰ç½®ä¿¡åº¦æ’åºå¹¶æ ‡è®°
        if len(detections) > 1:
            detections.sort(key=lambda x: x[1], reverse=True)  # æŒ‰ç½®ä¿¡åº¦é™åºæ’åº
            print(f"ğŸ¸ Multiple shuttlecocks detected: {len(detections)} objects")
            for i, (pos, conf) in enumerate(detections):
                print(f"   Shuttlecock {i+1}: position {pos}, confidence {conf:.3f}")

        return detections

    def get_buffer_info(self):
        """è·å–ç¼“å†²åŒºä¿¡æ¯"""
        return {
            'buffer_size': len(self.image_buffer1),
            'max_size': self.max_buffer_size,
            'is_processing': self.is_processing,
            'buffer_time_span': len(self.timestamp_buffer) / self.fps if self.timestamp_buffer else 0
        }

    def clear_buffer(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        if not self.is_processing:
            self.image_buffer1.clear()
            self.image_buffer2.clear()
            self.timestamp_buffer.clear()
            print("Image buffer cleared")


class StereoProcessor:
    """å¢å¼ºçš„åŒç›®è§†è§‰å¤„ç†å™¨ - æ·»åŠ è°ƒè¯•æ•°æ®è¿½è¸ªå’Œå¤šç›®æ ‡æ”¯æŒ"""

    def __init__(self):
        self.camera1_params = None
        self.camera2_params = None
        self.fundamental_matrix = None

        # è½¨è¿¹ç®¡ç†
        self.trajectory_manager = TrajectorySegmentManager()
        
        # å¤šç›®æ ‡è·Ÿè¸ªå™¨
        self.multi_tracker = MultiObjectTracker(max_objects=2, distance_threshold=100)

        # åœºåœ°è¿‡æ»¤å‚æ•° (cm) - æ‰©å¤§èŒƒå›´æ”¯æŒåœºåœ°å¤–é¢„æµ‹
        self.court_bounds = {
            'x_min': -500,  # æ‰©å¤§åˆ°åœºåœ°å¤–1.5ç±³
            'x_max': 500,
            'y_min': -900,  # æ‰©å¤§åˆ°åœºåœ°å¤–2ç±³
            'y_max': 900,
            'z_min': 0,
            'z_max': 800  # 8ç±³é«˜åº¦ä¸Šé™
        }

        # 3Dç‚¹å­˜å‚¨ - æ”¯æŒå¤šè½¨è¿¹
        self.all_3d_points = []
        self.all_timestamps = []
        self.track_data = {}  # track_id -> {'points': [], 'timestamps': []}

        # æ–°å¢ï¼šè°ƒè¯•æ•°æ®å­˜å‚¨
        self.rejected_points = []  # è¢«è¾¹ç•Œè¿‡æ»¤æ’é™¤çš„ç‚¹
        self.low_quality_points = []  # è´¨é‡è¯„ä¼°ä½çš„ç‚¹
        self.triangulation_failed_points = []  # ä¸‰è§’æµ‹é‡å¤±è´¥çš„ç‚¹å¯¹

        print("StereoProcessor initialized with debug tracking and multi-object support enabled")

    def load_camera_parameters(self, camera1_file, camera2_file):
        """åŠ è½½ç›¸æœºå‚æ•°"""
        try:
            self.camera1_params = self._load_camera_params(camera1_file)
            self.camera2_params = self._load_camera_params(camera2_file)
            self._compute_fundamental_matrix()
            print("Stereo camera parameters loaded successfully")
            return True
        except Exception as e:
            print(f"Error loading stereo parameters: {e}")
            return False

    def _load_camera_params(self, params_file):
        """ä»æ–‡ä»¶åŠ è½½ç›¸æœºå‚æ•°"""
        fs = cv2.FileStorage(params_file, cv2.FILE_STORAGE_READ)

        camera_matrix = fs.getNode("camera_matrix").mat()
        dist_coeffs = fs.getNode("distortion_coefficients").mat().flatten()
        rotation_vector = fs.getNode("rotation_vector").mat()
        translation_vector = fs.getNode("translation_vector").mat()

        fs.release()

        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
        camera_position = -np.dot(rotation_matrix.T, translation_vector)

        return {
            'camera_matrix': camera_matrix,
            'dist_coeffs': dist_coeffs,
            'rotation_vector': rotation_vector,
            'translation_vector': translation_vector,
            'rotation_matrix': rotation_matrix,
            'camera_position': camera_position
        }

    def _compute_fundamental_matrix(self):
        """è®¡ç®—åŸºç¡€çŸ©é˜µ"""
        if self.camera1_params is None or self.camera2_params is None:
            return

        try:
            R1 = self.camera1_params['rotation_matrix']
            R2 = self.camera2_params['rotation_matrix']
            t1 = self.camera1_params['translation_vector']
            t2 = self.camera2_params['translation_vector']

            R_rel = R2 @ R1.T
            t_rel = t2 - R_rel @ t1

            tx = np.array([[0, -t_rel[2, 0], t_rel[1, 0]],
                           [t_rel[2, 0], 0, -t_rel[0, 0]],
                           [-t_rel[1, 0], t_rel[0, 0], 0]])

            E = tx @ R_rel
            K1 = self.camera1_params['camera_matrix']
            K2 = self.camera2_params['camera_matrix']
            self.fundamental_matrix = np.linalg.inv(K2).T @ E @ np.linalg.inv(K1)

            print("Fundamental matrix computed successfully")

        except Exception as e:
            print(f"Error computing fundamental matrix: {e}")
            self.fundamental_matrix = None

    def process_batch_detections(self, detections_list1, detections_list2, timestamps):
        """æ‰¹é‡å¤„ç†æ£€æµ‹ç»“æœ - å¢å¼ºè°ƒè¯•è¿½è¸ªå’Œå¤šç›®æ ‡æ”¯æŒ"""
        if len(detections_list1) != len(detections_list2) != len(timestamps):
            print("Error: Detection lists and timestamps length mismatch")
            return []

        all_3d_points = []
        all_timestamps_3d = []

        # æ¸…ç©ºè°ƒè¯•æ•°æ®
        self.rejected_points = []
        self.low_quality_points = []
        self.triangulation_failed_points = []

        print(f"Processing {len(detections_list1)} frame pairs with multi-object tracking...")

        # å¤„ç†æ¯ä¸€å¸§çš„æ£€æµ‹ç»“æœ
        for i, (det1, det2, timestamp) in enumerate(zip(detections_list1, detections_list2, timestamps)):
            # åŒç›®åŒ¹é…
            matched_pairs = self._match_stereo_points(det1, det2)

            # å½“å‰å¸§çš„æ‰€æœ‰3Dæ£€æµ‹ç‚¹
            frame_3d_points = []
            
            # ä¸‰è§’æµ‹é‡
            for left_point, right_point, match_distance, match_conf in matched_pairs:
                point_3d = self._triangulate_point(left_point, right_point)

                if point_3d is None:
                    # è®°å½•ä¸‰è§’æµ‹é‡å¤±è´¥çš„ç‚¹å¯¹
                    self.triangulation_failed_points.append({
                        'left_point': left_point,
                        'right_point': right_point,
                        'timestamp': timestamp,
                        'frame_index': i,
                        'reason': 'triangulation_failed'
                    })
                    continue

                if self._is_point_in_bounds(point_3d):
                    frame_3d_points.append((point_3d, match_conf))
                    all_3d_points.append(point_3d)
                    all_timestamps_3d.append(timestamp)
                else:
                    # è®°å½•è¢«è¾¹ç•Œè¿‡æ»¤æ’é™¤çš„ç‚¹
                    self.rejected_points.append({
                        'point_3d': point_3d,
                        'timestamp': timestamp,
                        'frame_index': i,
                        'reason': 'out_of_bounds',
                        'match_confidence': match_conf,
                        'match_distance': match_distance
                    })
            
            # æ›´æ–°å¤šç›®æ ‡è·Ÿè¸ªå™¨
            if frame_3d_points:
                # å°†3Dç‚¹æŠ•å½±åˆ°2Dç”¨äºè·Ÿè¸ªï¼ˆä½¿ç”¨ç›¸æœº1çš„æŠ•å½±ï¼‰
                tracking_detections = []
                for point_3d, conf in frame_3d_points:
                    # ç®€åŒ–æŠ•å½±ï¼šä½¿ç”¨XYåæ ‡ä½œä¸º2Dä½ç½®
                    pos_2d = (int(point_3d[0] + 500), int(point_3d[1] + 500))  # åç§»åˆ°æ­£å€¼
                    tracking_detections.append((pos_2d, conf))
                
                # æ›´æ–°è·Ÿè¸ªå™¨
                active_tracks = self.multi_tracker.update(tracking_detections, timestamp)
                
                if len(active_tracks) > 1:
                    print(f"ğŸ“ Frame {i}: Tracking {len(active_tracks)} shuttlecocks")

        # å­˜å‚¨æ‰€æœ‰3Dç‚¹
        self.all_3d_points = all_3d_points
        self.all_timestamps = all_timestamps_3d

        # åˆ†æè·Ÿè¸ªç»“æœ
        tracks = self.multi_tracker.get_tracks()
        if len(tracks) > 1:
            print(f"ğŸ¸ Multiple shuttlecock tracks detected:")
            for track_id, track in tracks.items():
                print(f"   Track {track_id}: {len(track['positions'])} points, "
                      f"avg_conf={np.mean(track['confidences']):.3f}")

        print(f"Generated {len(all_3d_points)} valid 3D points from batch processing")
        print(f"Rejected {len(self.rejected_points)} out-of-bounds points")
        print(f"Failed triangulation for {len(self.triangulation_failed_points)} point pairs")

        return all_3d_points, all_timestamps_3d

    def get_best_trajectory_from_tracks(self, current_time):
        """ä»å¤šä¸ªè½¨è¿¹ä¸­é€‰æ‹©æœ€ä½³è½¨è¿¹è¿›è¡Œé¢„æµ‹"""
        tracks = self.multi_tracker.get_tracks()
        
        if not tracks:
            # æ²¡æœ‰è½¨è¿¹æ—¶ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•
            return self.find_best_trajectory_for_prediction(current_time)
        
        if len(tracks) == 1:
            # åªæœ‰ä¸€ä¸ªè½¨è¿¹
            track_id, track = list(tracks.items())[0]
            print(f"ğŸ“ Using single track {track_id} with {len(track['positions'])} points")
            return self.find_best_trajectory_for_prediction(current_time)
        
        # å¤šä¸ªè½¨è¿¹ï¼Œé€‰æ‹©æœ€ä½³çš„
        best_track_id, best_track = self.multi_tracker.get_best_track()
        if best_track_id is not None:
            print(f"ğŸ¯ Selected best track {best_track_id} from {len(tracks)} tracks")
            print(f"   Track quality: {len(best_track['positions'])} points, "
                  f"missing_frames={best_track['missing_frames']}")
            
            # è¿‡æ»¤è½¨è¿¹æ•°æ®å¹¶è½¬æ¢ä¸º3Dç‚¹
            track_3d_points = []
            track_timestamps = []
            
            # åœ¨æ‰€æœ‰3Dç‚¹ä¸­æŸ¥æ‰¾å±äºæ­¤è½¨è¿¹çš„ç‚¹
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨æ—¶é—´çª—å£åŒ¹é…
            track_start_time = best_track['created_at']
            track_end_time = best_track['timestamps'][-1] if best_track['timestamps'] else current_time
            
            for i, timestamp in enumerate(self.all_timestamps):
                if track_start_time <= timestamp <= track_end_time + 0.1:  # å…è®¸å°è¯¯å·®
                    track_3d_points.append(self.all_3d_points[i])
                    track_timestamps.append(timestamp)
            
            if len(track_3d_points) >= 5:
                print(f"ğŸ“Š Using {len(track_3d_points)} 3D points from best track")
                confidence = min(1.0, len(track_3d_points) / 20.0)  # åŸºäºç‚¹æ•°çš„ç½®ä¿¡åº¦
                return track_3d_points, track_timestamps, confidence
        
        # å¦‚æœæ— æ³•ä»è½¨è¿¹è·å–è¶³å¤Ÿæ•°æ®ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        print("âš ï¸ Falling back to traditional trajectory selection")
        return self.find_best_trajectory_for_prediction(current_time)

    def _match_stereo_points(self, detections_left, detections_right, epipolar_threshold=18.0):
        """åŸºäºæçº¿çº¦æŸåŒ¹é…åŒç›®ç‚¹"""
        if not detections_left or not detections_right or self.fundamental_matrix is None:
            return []

        matches = []

        for left_point, left_conf in detections_left:
            point_homo = np.array([left_point[0], left_point[1], 1])
            epipolar_line = self.fundamental_matrix @ point_homo

            best_match = None
            min_distance = float('inf')
            best_conf = 0

            for right_point, right_conf in detections_right:
                distance = abs(epipolar_line[0] * right_point[0] +
                               epipolar_line[1] * right_point[1] +
                               epipolar_line[2]) / np.sqrt(epipolar_line[0] ** 2 + epipolar_line[1] ** 2)

                if distance < epipolar_threshold and distance < min_distance:
                    min_distance = distance
                    best_match = right_point
                    best_conf = (left_conf + right_conf) / 2

            if best_match is not None:
                matches.append((left_point, best_match, min_distance, best_conf))

        return matches

    def _triangulate_point(self, point1, point2):
        """ä¸‰è§’æµ‹é‡è®¡ç®—3Dç‚¹"""
        if self.camera1_params is None or self.camera2_params is None:
            return None

        try:
            point1_normalized = cv2.undistortPoints(
                np.array([point1], dtype=np.float32),
                self.camera1_params['camera_matrix'],
                self.camera1_params['dist_coeffs']
            )

            point2_normalized = cv2.undistortPoints(
                np.array([point2], dtype=np.float32),
                self.camera2_params['camera_matrix'],
                self.camera2_params['dist_coeffs']
            )

            ray1_dir = np.array([
                point1_normalized[0][0][0],
                point1_normalized[0][0][1],
                1.0
            ])

            ray2_dir = np.array([
                point2_normalized[0][0][0],
                point2_normalized[0][0][1],
                1.0
            ])

            ray1_dir_world = self.camera1_params['rotation_matrix'].T @ ray1_dir
            ray2_dir_world = self.camera2_params['rotation_matrix'].T @ ray2_dir

            ray1_dir_world = ray1_dir_world / np.linalg.norm(ray1_dir_world)
            ray2_dir_world = ray2_dir_world / np.linalg.norm(ray2_dir_world)

            camera1_position = self.camera1_params['camera_position']
            camera2_position = self.camera2_params['camera_position']

            n = np.cross(ray1_dir_world.flatten(), ray2_dir_world.flatten())

            if np.linalg.norm(n) < 1e-10:
                return None

            n1 = np.cross(ray1_dir_world.flatten(), n)
            n2 = np.cross(ray2_dir_world.flatten(), n)

            c1 = camera1_position.flatten()
            c2 = camera2_position.flatten()

            t1 = np.dot((c2 - c1), n2) / np.dot(ray1_dir_world.flatten(), n2)
            t2 = np.dot((c1 - c2), n1) / np.dot(ray2_dir_world.flatten(), n1)

            p1 = c1 + t1 * ray1_dir_world.flatten()
            p2 = c2 + t2 * ray2_dir_world.flatten()

            point_3d = (p1 + p2) / 2

            return point_3d

        except Exception as e:
            return None

    def _is_point_in_bounds(self, point_3d):
        """æ£€æŸ¥3Dç‚¹æ˜¯å¦åœ¨æ‰©å±•è¾¹ç•Œå†…"""
        if point_3d is None or len(point_3d) < 3:
            return False

        x, y, z = point_3d[0], point_3d[1], point_3d[2]

        return (self.court_bounds['x_min'] <= x <= self.court_bounds['x_max'] and
                self.court_bounds['y_min'] <= y <= self.court_bounds['y_max'] and
                self.court_bounds['z_min'] <= z <= self.court_bounds['z_max'])

    def find_best_trajectory_for_prediction(self, current_time):
        """æ‰¾åˆ°æœ€é€‚åˆé¢„æµ‹çš„è½¨è¿¹ç‰‡æ®µ - è®°å½•è¢«æ’é™¤çš„ä½è´¨é‡ç‚¹"""
        if len(self.all_3d_points) < 5:
            return None, None, 0.0

        # åœ¨å¯»æ‰¾æœ€ä½³è½¨è¿¹ä¹‹å‰ï¼Œè®°å½•æ‰€æœ‰ä¸ç¬¦åˆè´¨é‡è¦æ±‚çš„ç‚¹
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„è´¨é‡è¯„ä¼°é€»è¾‘
        for i, (point, timestamp) in enumerate(zip(self.all_3d_points, self.all_timestamps)):
            # ç®€å•çš„è´¨é‡æ£€æŸ¥ç¤ºä¾‹
            if i > 0:
                prev_point = self.all_3d_points[i - 1]
                prev_time = self.all_timestamps[i - 1]

                # æ£€æŸ¥è·ç¦»å’Œæ—¶é—´é—´éš”çš„åˆç†æ€§
                distance = np.linalg.norm(point - prev_point)
                time_diff = timestamp - prev_time

                if time_diff > 0:
                    velocity = distance / time_diff
                    # å¦‚æœé€Ÿåº¦å¼‚å¸¸é«˜ï¼Œæ ‡è®°ä¸ºä½è´¨é‡ç‚¹
                    if velocity > 2000:  # 20m/s
                        self.low_quality_points.append({
                            'point_3d': point,
                            'timestamp': timestamp,
                            'reason': 'high_velocity',
                            'velocity': velocity,
                            'distance': distance,
                            'time_diff': time_diff
                        })

        best_points, best_timestamps, confidence = self.trajectory_manager.find_best_trajectory_segment(
            self.all_3d_points, self.all_timestamps, current_time
        )

        return best_points, best_timestamps, confidence

    def get_debug_data(self):
        """è·å–è°ƒè¯•æ•°æ®"""
        return {
            'all_valid_points': self.all_3d_points,
            'all_timestamps': self.all_timestamps,
            'rejected_points': self.rejected_points,
            'low_quality_points': self.low_quality_points,
            'triangulation_failed_points': self.triangulation_failed_points
        }

    def reset(self):
        """é‡ç½®å¤„ç†å™¨çŠ¶æ€"""
        self.all_3d_points = []
        self.all_timestamps = []
        self.rejected_points = []
        self.low_quality_points = []
        self.triangulation_failed_points = []
        print("StereoProcessor reset")