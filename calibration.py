import cv2
import numpy as np
import os
import time
from ultralytics import YOLO
from utils import config


class BadmintonCalibrator:
    """ç¾½æ¯›çƒç›¸æœºæ ‡å®šç±»"""

    def __init__(self, camera_params_file, yolo_model_path, device='cpu'):
        """
        åˆå§‹åŒ–æ ‡å®šå™¨

        å‚æ•°:
            camera_params_file: å†…å‚æ ‡å®šæ–‡ä»¶è·¯å¾„
            yolo_model_path: YOLOæ£€æµ‹æ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ('cpu' æˆ– 'cuda')
        """
        self.load_camera_params(camera_params_file)
        self.court_3d_points, self.court_point_labels, self.merged_3d_points, self.merged_point_labels = self.setup_court_points()
        self.current_image = None
        self.matched_corners = {}
        self.yolo_model = YOLO(yolo_model_path)
        self.device = device
        self.homography_matrix = None
        self.manual_corners = []  # å­˜å‚¨æ‰‹åŠ¨é€‰æ‹©çš„å››ä¸ªç‚¹
        self.camera_params_file = camera_params_file

    def load_camera_params(self, params_file):
        """ä»æ–‡ä»¶åŠ è½½ç›¸æœºå†…å‚"""
        fs = cv2.FileStorage(params_file, cv2.FILE_STORAGE_READ)
        self.camera_matrix = fs.getNode("camera_matrix").mat()
        self.dist_coeffs = fs.getNode("distortion_coefficients").mat().flatten()
        self.image_width = int(fs.getNode("image_width").real() or 1280)
        self.image_height = int(fs.getNode("image_height").real() or 720)
        fs.release()

        print(f"Camera parameters loaded from: {params_file}")
        print(f"Camera matrix: \n{self.camera_matrix}")
        print(f"Distortion coefficients: {self.dist_coeffs}")

    def setup_court_points(self):
        """è®¾ç½®åœºåœ°3Dç‚¹åæ ‡"""
        # ä»…ä½¿ç”¨è¿‘åŠåœºç‚¹ (Y=0 åˆ° 670)
        pts = np.array([
            # åº•çº¿åŒºåŸŸ (Y = 0 to 4)
            [0, 0, 0], [610, 0, 0],  # å¤–è§’ç‚¹
            [4, 4, 0], [606, 4, 0],  # å†…è§’ç‚¹
            [46, 4, 0], [50, 4, 0],  # å·¦å•æ‰“è¾¹çº¿äº¤ç‚¹
            [303, 4, 0], [307, 4, 0],  # ä¸­çº¿äº¤ç‚¹
            [560, 4, 0], [564, 4, 0],  # å³å•æ‰“è¾¹çº¿äº¤ç‚¹

            # åŒæ‰“åå‘çƒçº¿åŒºåŸŸ (Y = 76 to 80)
            [4, 76, 0], [4, 80, 0],  # å·¦åŒæ‰“è¾¹çº¿äº¤ç‚¹
            [46, 76, 0], [50, 76, 0], [46, 80, 0], [50, 80, 0],  # å·¦å•æ‰“åå­—äº¤ç‚¹
            [303, 76, 0], [307, 76, 0], [303, 80, 0], [307, 80, 0],  # ä¸­çº¿åå­—äº¤ç‚¹
            [560, 76, 0], [564, 76, 0], [560, 80, 0], [564, 80, 0],  # å³å•æ‰“åå­—äº¤ç‚¹
            [606, 76, 0], [606, 80, 0],  # å³åŒæ‰“è¾¹çº¿äº¤ç‚¹

            # å‰å‘çƒçº¿åŒºåŸŸ (Y = 468 to 472)
            [4, 468, 0], [4, 472, 0],  # å·¦åŒæ‰“è¾¹çº¿äº¤ç‚¹
            [46, 468, 0], [50, 468, 0], [46, 472, 0], [50, 472, 0],  # å·¦å•æ‰“åå­—äº¤ç‚¹
            [303, 468, 0], [307, 468, 0], [303, 472, 0], [307, 472, 0],  # ä¸­çº¿åå­—äº¤ç‚¹
            [560, 468, 0], [564, 468, 0], [560, 472, 0], [564, 472, 0],  # å³å•æ‰“åå­—äº¤ç‚¹
            [606, 468, 0], [606, 472, 0],  # å³åŒæ‰“è¾¹çº¿äº¤ç‚¹
        ], dtype=np.float32)

        # ä¸ºæ¯ä¸ªç‚¹æ·»åŠ æ ‡ç­¾ï¼Œä¾¿äºåŒ¹é…å’ŒéªŒè¯
        labels = [
            "Bottom Left Outer", "Bottom Right Outer",
            "Bottom Left Inner", "Bottom Right Inner",
            "Left Singles Bottom Left", "Left Singles Bottom Right",
            "Center Line Bottom Left", "Center Line Bottom Right",
            "Right Singles Bottom Left", "Right Singles Bottom Right",

            "Left Doubles Back Service Bottom", "Left Doubles Back Service Top",
            "Left Singles Back Service Bottom Left", "Left Singles Back Service Bottom Right",
            "Left Singles Back Service Top Left", "Left Singles Back Service Top Right",
            "Center Line Back Service Bottom Left", "Center Line Back Service Bottom Right",
            "Center Line Back Service Top Left", "Center Line Back Service Top Right",
            "Right Singles Back Service Bottom Left", "Right Singles Back Service Bottom Right",
            "Right Singles Back Service Top Left", "Right Singles Back Service Top Right",
            "Right Doubles Back Service Bottom", "Right Doubles Back Service Top",

            "Left Doubles Front Service Bottom", "Left Doubles Front Service Top",
            "Left Singles Front Service Bottom Left", "Left Singles Front Service Bottom Right",
            "Left Singles Front Service Top Left", "Left Singles Front Service Top Right",
            "Center Line Front Service Bottom Left", "Center Line Front Service Bottom Right",
            "Center Line Front Service Top Left", "Center Line Front Service Top Right",
            "Right Singles Front Service Bottom Left", "Right Singles Front Service Bottom Right",
            "Right Singles Front Service Top Left", "Right Singles Front Service Top Right",
            "Right Doubles Front Service Bottom", "Right Doubles Front Service Top"
        ]

        # åˆ›å»ºåˆå¹¶åçš„ç‚¹é›†ï¼ˆå°†ç›¸è¿‘çš„åå­—è§’ç‚¹åˆå¹¶ä¸ºä¸€ä¸ªç‚¹ï¼‰
        merged_pts = []
        merged_labels = []

        # æ·»åŠ å››ä¸ªå¤–è§’ç‚¹ - è¿™äº›ä¼šè¢«æ‰‹åŠ¨é€‰æ‹©
        merged_pts.append([0, 0, 0])
        merged_labels.append("(0, 0, 0)")
        merged_pts.append([610, 0, 0])
        merged_labels.append("(610, 0, 0)")
        merged_pts.append([606, 472, 0])
        merged_labels.append("(606, 472, 0)")
        merged_pts.append([4, 472, 0])
        merged_labels.append("(4, 472, 0)")

        # æ·»åŠ é¢å¤–çš„ä¸¤ä¸ªç‚¹
        merged_pts.append([610, 472, 0])
        merged_labels.append("(610, 472, 0)")
        merged_pts.append([0, 472, 0])
        merged_labels.append("(0, 472, 0)")

        # æ·»åŠ å·¦å•æ‰“è¾¹çº¿åº•ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([48, 4, 0])
        merged_labels.append("(48, 4, 0)")

        # æ·»åŠ ä¸­çº¿åº•ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([305, 4, 0])
        merged_labels.append("(305, 4, 0)")

        # æ·»åŠ å³å•æ‰“è¾¹çº¿åº•ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([562, 4, 0])
        merged_labels.append("(562, 4, 0)")

        # æ·»åŠ å·¦åŒæ‰“åå‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰- ä» (4, 76), (4, 80) åˆ° (4, 78)
        merged_pts.append([4, 78, 0])
        merged_labels.append("(4, 78, 0)")

        # æ·»åŠ å·¦å•æ‰“åå‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([48, 78, 0])
        merged_labels.append("(48, 78, 0)")

        # æ·»åŠ ä¸­çº¿åå‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([305, 78, 0])
        merged_labels.append("(305, 78, 0)")

        # æ·»åŠ å³å•æ‰“åå‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([562, 78, 0])
        merged_labels.append("(562, 78, 0)")

        # æ·»åŠ å³åŒæ‰“åå‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰- ä» (606, 76), (606, 80) åˆ° (606, 78)
        merged_pts.append([606, 78, 0])
        merged_labels.append("(606, 78, 0)")

        # æ·»åŠ å·¦åŒæ‰“å‰å‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰- ä» (4, 468), (4, 472) åˆ° (4, 470)
        # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å°† (4, 472) åˆå¹¶è¿›æ¥ï¼Œå› ä¸ºå®ƒæ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ç‚¹ä¹‹ä¸€
        merged_pts.append([4, 470, 0])
        merged_labels.append("(4, 470, 0)")

        # æ·»åŠ å·¦å•æ‰“å‰å‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([48, 470, 0])
        merged_labels.append("(48, 470, 0)")

        # æ·»åŠ ä¸­çº¿å‰å‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([305, 470, 0])
        merged_labels.append("(305, 470, 0)")

        # æ·»åŠ å³å•æ‰“å‰å‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        merged_pts.append([562, 470, 0])
        merged_labels.append("(562, 470, 0)")

        # æ·»åŠ å³åŒæ‰“å‰å‘çƒçº¿äº¤ç‚¹ï¼ˆåˆå¹¶ï¼‰
        # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å°† (606, 472) åˆå¹¶è¿›æ¥ï¼Œå› ä¸ºå®ƒæ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ç‚¹ä¹‹ä¸€
        merged_pts.append([606, 470, 0])
        merged_labels.append("(606, 470, 0)")

        return pts, labels, np.array(merged_pts, dtype=np.float32), merged_labels

    def zoom_point_selection(self, image, roi_x, roi_y, zoom_radius, point_name):
        """æ”¾å¤§åŒºåŸŸè¿›è¡Œç²¾ç¡®ç‚¹é€‰æ‹©ï¼Œä½¿ç”¨å·¦é”®åœ¨æ”¾å¤§åŒºåŸŸä¸­é€‰æ‹©ç‚¹"""
        # æå–æ”¾å¤§åŒºåŸŸ
        h, w = image.shape[:2]
        x1 = max(0, roi_x - zoom_radius)
        y1 = max(0, roi_y - zoom_radius)
        x2 = min(w, roi_x + zoom_radius)
        y2 = min(h, roi_y + zoom_radius)

        zoom_img = image[y1:y2, x1:x2].copy()

        # åˆ›å»ºæ”¾å¤§çª—å£
        window = f"Zoom Selection - {point_name}"
        cv2.namedWindow(window, cv2.WINDOW_NORMAL)
        cv2.resizeWindow(window, 800, 800)

        selected_point = [None]

        def mouse_cb(event, x, y, flags, param):
            if event == cv2.EVENT_LBUTTONDOWN:
                selected_point[0] = (x, y)

        cv2.setMouseCallback(window, mouse_cb)

        # æ˜¾ç¤ºæŒ‡å¯¼ä¿¡æ¯
        disp = zoom_img.copy()
        text = f"Click exactly on the {point_name}"
        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)
        cv2.rectangle(disp, (10, 30), (10 + text_size[0] + 10, 70), (0, 0, 0), -1)
        cv2.putText(disp, text, (15, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

        # æ·»åŠ ä¸€ä¸ªåå­—çº¿æˆ–åœ†å½¢æ ‡è®°åœ¨å›¾åƒä¸­å¿ƒï¼Œå¸®åŠ©å®šä½
        center_x, center_y = zoom_img.shape[1] // 2, zoom_img.shape[0] // 2
        cv2.line(disp, (center_x - 10, center_y), (center_x + 10, center_y), (0, 255, 255), 1)
        cv2.line(disp, (center_x, center_y - 10), (center_x, center_y + 10), (0, 255, 255), 1)

        while True:
            # å¦‚æœæœ‰é€‰æ‹©çš„ç‚¹ï¼Œç”»å‡ºæ¥
            if selected_point[0] is not None:
                disp = zoom_img.copy()
                cv2.putText(disp, text, (15, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.circle(disp, selected_point[0], 5, (0, 255, 255), -1)
                cv2.putText(disp, "Press SPACE to confirm or ESC to re-select",
                            (15, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)

            cv2.imshow(window, disp)
            key = cv2.waitKey(1) & 0xFF

            if key == 27:  # ESC
                selected_point[0] = None  # é‡ç½®
            elif key == ord(' ') and selected_point[0] is not None:  # SPACE
                break

        cv2.destroyWindow(window)

        # å¦‚æœç”¨æˆ·é€‰æ‹©äº†ç‚¹ï¼Œè½¬æ¢å›åŸå›¾åæ ‡
        if selected_point[0] is not None:
            global_x = x1 + selected_point[0][0]
            global_y = y1 + selected_point[0][1]
            return (global_x, global_y)

        return None

    def select_initial_court_boundary(self, image):
        """è®©æ“ä½œå‘˜ä¾æ¬¡ç²¾ç¡®ç‚¹å‡»å››ä¸ªåœºåœ°è§’ç‚¹ï¼Œä½¿ç”¨å³é”®ç›´æ¥æ˜¾ç¤ºæ”¾å¤§åŒºåŸŸ"""
        points = []
        window = "Select Court Boundary"
        cv2.namedWindow(window, cv2.WINDOW_NORMAL)

        # è®¾ç½®æç¤ºæ–‡æœ¬
        point_names = [
            "Bottom Left Outer (0,0)",
            "Bottom Right Outer (610,0)",
            "Top Right Inner (606,472)",
            "Top Left Inner (4,472)"
        ]

        current_point_idx = 0
        zoom_radius = int(min(image.shape[0], image.shape[1]) * 0.1)

        # ä½¿ç”¨é—­åŒ…ä½¿selfåœ¨å›è°ƒä¸­å¯ç”¨
        calibrator = self

        def mouse_cb(event, x, y, flags, param):
            nonlocal current_point_idx, calibrator

            # å³é”®ç‚¹å‡» - ç›´æ¥æ¿€æ´»æ”¾å¤§åŒºåŸŸå¹¶é€‰æ‹©ç‚¹
            if event == cv2.EVENT_RBUTTONDOWN and current_point_idx < 4:
                # ç›´æ¥æ˜¾ç¤ºæ”¾å¤§è§†å›¾å¹¶è¿›è¡Œç²¾ç¡®é€‰æ‹©
                selected_point = calibrator.zoom_point_selection(
                    image,
                    x,
                    y,
                    zoom_radius,
                    point_names[current_point_idx]
                )

                if selected_point is not None:
                    points.append(selected_point)
                    current_point_idx += 1

        cv2.setMouseCallback(window, mouse_cb)

        while current_point_idx < 4:
            disp = image.copy()

            # æ˜¾ç¤ºå½“å‰éœ€è¦ç‚¹å‡»çš„ä½ç½®æç¤º
            text = f"Right-click near {point_names[current_point_idx]} to zoom and select"
            text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)
            cv2.rectangle(disp, (45, 35), (45 + text_size[0] + 10, 55 + text_size[1]), (0, 0, 0), -1)
            cv2.putText(disp, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

            # æ˜¾ç¤ºå·²é€‰æ‹©çš„ç‚¹
            for i, pt in enumerate(points):
                cv2.circle(disp, pt, 8, (0, 255, 255), -1)
                cv2.putText(disp, f"{i + 1}", (pt[0] + 10, pt[1] + 10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.7, (0, 255, 255), 2)

            cv2.imshow(window, disp)
            key = cv2.waitKey(1) & 0xFF

            # ESCé”®æ¸…é™¤æ‰€æœ‰ç‚¹é‡æ–°å¼€å§‹
            if key == 27:
                points = []
                current_point_idx = 0

        cv2.destroyWindow(window)

        # ä¿å­˜æ‰‹åŠ¨é€‰æ‹©çš„å››ä¸ªç‚¹ä¾›åç»­æ ‡å®šä½¿ç”¨
        self.manual_corners = points.copy()

        # åˆ›å»ºæ©ç å›¾åƒ
        mask = np.zeros(image.shape[:2], np.uint8)
        if len(points) == 4:
            cv2.fillPoly(mask, [np.array(points)], 255)

        # ç•¥å¾®å‘å¤–æ‰©å±•å¤šè¾¹å½¢ï¼ˆæ‰©å±•5%ï¼‰
        expanded_points = []
        if len(points) == 4:
            # è®¡ç®—ä¸­å¿ƒç‚¹
            center_x = sum(p[0] for p in points) / 4
            center_y = sum(p[1] for p in points) / 4

            # å‘å¤–æ‰©å±•
            for p in points:
                vector_x = p[0] - center_x
                vector_y = p[1] - center_y
                expanded_points.append((
                    int(p[0] + vector_x * 0.05),
                    int(p[1] + vector_y * 0.15)
                ))

            # æ›´æ–°æ©ç 
            mask = np.zeros(image.shape[:2], np.uint8)
            cv2.fillPoly(mask, [np.array(expanded_points)], 255)

        # åˆ›å»ºå¸¦é®ç½©çš„å›¾åƒ
        masked_img = image.copy()
        # å°†é®ç½©å¤–åŒºåŸŸè¿›è¡Œé©¬èµ›å…‹å¤„ç†
        outside_mask = cv2.bitwise_not(mask)
        # åˆ›å»ºé©¬èµ›å…‹æ•ˆæœ
        mosaic_img = image.copy()
        mosaic_block_size = 20
        h, w = image.shape[:2]

        for i in range(0, h, mosaic_block_size):
            for j in range(0, w, mosaic_block_size):
                if i + mosaic_block_size <= h and j + mosaic_block_size <= w:
                    roi = image[i:i + mosaic_block_size, j:j + mosaic_block_size]
                    color = roi.mean(axis=(0, 1))
                    mosaic_img[i:i + mosaic_block_size, j:j + mosaic_block_size] = color

        # å°†é©¬èµ›å…‹åº”ç”¨åˆ°é®ç½©å¤–åŒºåŸŸ
        masked_img = cv2.bitwise_and(masked_img, masked_img, mask=mask)
        mosaic_outside = cv2.bitwise_and(mosaic_img, mosaic_img, mask=outside_mask)
        final_img = cv2.add(masked_img, mosaic_outside)

        # è®¡ç®—åˆå§‹å•åº”æ€§çŸ©é˜µç”¨äºè¾…åŠ©åŒ¹é…
        court_corners_2d = np.array([
            [0, 0],
            [610, 0],
            [606, 472],
            [4, 472]
        ], dtype=np.float32)

        self.initial_homography = cv2.findHomography(np.array(points), court_corners_2d)[0]

        return final_img, mask, points

    def capture_and_process_frames(self, video_path, num_frames=30):
        """ä»è§†é¢‘ä¸­æå–è¿ç»­å¤šå¸§å¹¶å¤„ç†"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print("Error: Could not open video file")
            return None, None, None

        # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames <= 0:
            print("Error: Could not determine total frames in video")
            cap.release()
            return None, None, None

        # è·å–ç¬¬ä¸€å¸§ç”¨äºåˆå§‹è¾¹ç•Œé€‰æ‹©
        ret, first_frame = cap.read()
        if not ret:
            print("Error: Could not read first frame")
            cap.release()
            return None, None, None

        # è®©ç”¨æˆ·é€‰æ‹©åˆå§‹åœºåœ°è¾¹ç•Œ
        masked_first_frame, mask, boundary_points = self.select_initial_court_boundary(first_frame)

        # é€‰æ‹©èµ·å§‹å¸§ä½ç½®ï¼ˆç”¨æˆ·å¯ä»¥è°ƒæ•´ï¼‰
        starting_frame = 0
        frame_selection_window = "Select Starting Frame"
        cv2.namedWindow(frame_selection_window, cv2.WINDOW_NORMAL)

        def on_trackbar_change(val):
            cap.set(cv2.CAP_PROP_POS_FRAMES, val)
            ret, frame = cap.read()
            if ret:
                # åº”ç”¨ç›¸åŒçš„æ©ç å¤„ç†
                masked_frame = frame.copy()
                masked_frame = cv2.bitwise_and(masked_frame, masked_frame, mask=mask)

                # åˆ›å»ºé©¬èµ›å…‹æ•ˆæœ
                mosaic_frame = frame.copy()
                mosaic_block_size = 20
                h, w = frame.shape[:2]

                for y in range(0, h, mosaic_block_size):
                    for x in range(0, w, mosaic_block_size):
                        if y + mosaic_block_size <= h and x + mosaic_block_size <= w:
                            roi = frame[y:y + mosaic_block_size, x:x + mosaic_block_size]
                            color = roi.mean(axis=(0, 1))
                            mosaic_frame[y:y + mosaic_block_size, x:x + mosaic_block_size] = color

                outside_mask = cv2.bitwise_not(mask)
                mosaic_outside = cv2.bitwise_and(mosaic_frame, mosaic_frame, mask=outside_mask)
                final_frame = cv2.add(masked_frame, mosaic_outside)

                # æ˜¾ç¤ºæ¡†æ¶ç¼–å·
                cv2.putText(final_frame, f"Frame: {val}", (50, 50), cv2.FONT_HERSHEY_SIMPLEX,
                            1, (0, 255, 255), 2)
                cv2.imshow(frame_selection_window, final_frame)

        # åˆ›å»ºè½¨è¿¹æ¡
        cv2.createTrackbar("Frame", frame_selection_window, 0, max(0, total_frames - num_frames - 1),
                           on_trackbar_change)

        # åˆå§‹æ˜¾ç¤º
        on_trackbar_change(0)

        # ç­‰å¾…ç”¨æˆ·é€‰æ‹©å¹¶ç¡®è®¤
        while True:
            key = cv2.waitKey(1) & 0xFF
            if key == ord(' '):  # ç©ºæ ¼ç¡®è®¤
                starting_frame = cv2.getTrackbarPos("Frame", frame_selection_window)
                break
            elif key == 27:  # ESCå–æ¶ˆ
                starting_frame = 0
                break

        cv2.destroyWindow(frame_selection_window)

        # å°†è§†é¢‘å®šä½åˆ°é€‰æ‹©çš„èµ·å§‹å¸§
        cap.set(cv2.CAP_PROP_POS_FRAMES, starting_frame)

        processed_frames = []
        all_detected_corners = []

        # å±•ç¤ºå¤„ç†è¿›åº¦çš„çª—å£
        progress_window = "Processing Frames"
        cv2.namedWindow(progress_window, cv2.WINDOW_NORMAL)

        for i in range(num_frames):
            # è¯»å–è¿ç»­å¸§
            ret, frame = cap.read()
            if not ret:
                print(f"Warning: Could only read {i} frames of {num_frames} requested")
                break

            # åº”ç”¨ç›¸åŒçš„æ©ç å’Œé©¬èµ›å…‹å¤„ç†
            masked_frame = frame.copy()
            masked_frame = cv2.bitwise_and(masked_frame, masked_frame, mask=mask)

            # åˆ›å»ºé©¬èµ›å…‹æ•ˆæœç”¨äºé®ç½©å¤–åŒºåŸŸ
            mosaic_frame = frame.copy()
            mosaic_block_size = 20
            h, w = frame.shape[:2]

            for y in range(0, h, mosaic_block_size):
                for x in range(0, w, mosaic_block_size):
                    if y + mosaic_block_size <= h and x + mosaic_block_size <= w:
                        roi = frame[y:y + mosaic_block_size, x:x + mosaic_block_size]
                        color = roi.mean(axis=(0, 1))
                        mosaic_frame[y:y + mosaic_block_size, x:x + mosaic_block_size] = color

            outside_mask = cv2.bitwise_not(mask)
            mosaic_outside = cv2.bitwise_and(mosaic_frame, mosaic_frame, mask=outside_mask)
            final_frame = cv2.add(masked_frame, mosaic_outside)

            # ä½¿ç”¨YOLOv8æ£€æµ‹è§’ç‚¹
            corners = self.detect_court_corners_yolov8(final_frame)
            all_detected_corners.extend(corners)

            # ä¿å­˜å¤„ç†åçš„å¸§
            processed_frames.append(final_frame)

            # æ˜¾ç¤ºå¤„ç†è¿›åº¦å¹¶æ”¹è¿›æ–‡æœ¬å¯è§æ€§
            progress_img = final_frame.copy()
            # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºæ£€æµ‹åˆ°çš„è§’ç‚¹
            for pt in corners:
                cv2.circle(progress_img, pt, 4, (0, 0, 255), -1)  # å°†ç‚¹å¤§å°æ”¹å›4

            # åœ¨æ–‡æœ¬åæ·»åŠ èƒŒæ™¯ä»¥æé«˜å¯è§æ€§
            progress_text = f"Processing frame {i + 1}/{num_frames}"
            text_size, _ = cv2.getTextSize(progress_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)
            cv2.rectangle(progress_img, (45, 35), (45 + text_size[0] + 10, 55 + text_size[1]), (0, 0, 0), -1)
            cv2.putText(progress_img, progress_text,
                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.imshow(progress_window, progress_img)
            cv2.waitKey(10)  # çŸ­æš‚æ˜¾ç¤º

        cv2.destroyWindow(progress_window)
        cap.release()

        # ä½¿ç”¨è¾ƒå¤§çš„èšç±»é˜ˆå€¼åˆå¹¶è§’ç‚¹
        consolidated_corners = self.consolidate_corner_points(all_detected_corners, threshold=30)  # èšç±»é˜ˆå€¼æ”¹å›30

        return processed_frames, consolidated_corners, boundary_points

    def detect_court_corners_yolov8(self, image):
        """ä½¿ç”¨YOLOv8æ£€æµ‹åœºåœ°è§’ç‚¹"""
        results = self.yolo_model(image)
        corners = []

        for r in results:
            # æ£€æŸ¥æ˜¯å¦æœ‰å…³é”®ç‚¹ç»“æœ
            if hasattr(r, 'keypoints') and r.keypoints is not None:
                kpts = r.keypoints.xy.cpu().numpy() if hasattr(r.keypoints.xy, "cpu") else r.keypoints.xy
                for kp_list in kpts:
                    for kp in kp_list:
                        if not np.isnan(kp).any():  # æ’é™¤æ— æ•ˆç‚¹
                            corners.append((int(kp[0]), int(kp[1])))

        return corners

    def consolidate_corner_points(self, corners, threshold=30):
        """åˆå¹¶ç›¸è¿‘çš„è§’ç‚¹"""
        if not corners:
            return []

        clusters = []
        for point in corners:
            # æŸ¥æ‰¾ç‚¹æ‰€å±çš„èšç±»
            found_cluster = False
            for cluster in clusters:
                for cluster_point in cluster:
                    # å¦‚æœç‚¹ä¸èšç±»ä¸­çš„ä»»ä¸€ç‚¹è·ç¦»å°äºé˜ˆå€¼ï¼Œå°†å…¶åŠ å…¥è¯¥èšç±»
                    if np.linalg.norm(np.array(point) - np.array(cluster_point)) < threshold:
                        cluster.append(point)
                        found_cluster = True
                        break
                if found_cluster:
                    break

            # å¦‚æœæœªæ‰¾åˆ°æ‰€å±èšç±»ï¼Œåˆ›å»ºæ–°èšç±»
            if not found_cluster:
                clusters.append([point])

        # å¯¹æ¯ä¸ªèšç±»è®¡ç®—ä¸­å¿ƒç‚¹
        consolidated_points = []
        for cluster in clusters:
            if len(cluster) >= 1:
                x_mean = int(np.mean([p[0] for p in cluster]))
                y_mean = int(np.mean([p[1] for p in cluster]))
                consolidated_points.append((x_mean, y_mean))

        # å°†æ‰‹åŠ¨é€‰æ‹©çš„å››ä¸ªè§’ç‚¹æ·»åŠ åˆ°åˆå¹¶åçš„ç‚¹é›†
        for corner in self.manual_corners:
            if corner not in consolidated_points:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æœ‰éå¸¸æ¥è¿‘çš„ç‚¹
                is_close = False
                for pt in consolidated_points:
                    if np.linalg.norm(np.array(corner) - np.array(pt)) < threshold:
                        is_close = True
                        break
                if not is_close:
                    consolidated_points.append(corner)

        return consolidated_points

    def match_corners_to_3d_points(self, corners, boundary_points):
        """ä½¿ç”¨åˆå§‹å•åº”æ€§çŸ©é˜µè¾…åŠ©åŒ¹é…æ£€æµ‹åˆ°çš„è§’ç‚¹ä¸3Dåæ ‡"""
        if not corners or self.initial_homography is None:
            return {}

        matched = {}

        # ä½¿ç”¨å•åº”æ€§çŸ©é˜µå°†3Dç‚¹æŠ•å½±åˆ°å›¾åƒå¹³é¢
        projected_3d_points = {}
        for i, (point_3d, label) in enumerate(zip(self.merged_3d_points, self.merged_point_labels)):
            # å°†3Dç‚¹æŠ•å½±åˆ°2Då¹³é¢ (z=0ï¼Œæ‰€ä»¥åªéœ€è€ƒè™‘x,y)
            pt_2d = np.array([point_3d[0], point_3d[1]], dtype=np.float32).reshape(1, 1, 2)
            projected_pt = cv2.perspectiveTransform(pt_2d, np.linalg.inv(self.initial_homography))
            projected_pt = (int(projected_pt[0][0][0]), int(projected_pt[0][0][1]))
            projected_3d_points[i] = (projected_pt, point_3d, label)

        # ä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„è§’ç‚¹æ‰¾æœ€è¿‘çš„3Dç‚¹
        for corner in corners:
            min_dist = float('inf')
            best_match = None

            for idx, (projected_pt, point_3d, label) in projected_3d_points.items():
                dist = np.linalg.norm(np.array(corner) - np.array(projected_pt))
                if dist < min_dist and dist < 50:  # è®¾ç½®æœ€å¤§åŒ¹é…è·ç¦»
                    min_dist = dist
                    best_match = (idx, point_3d, label)

            if best_match:
                matched[best_match[0]] = (corner, best_match[1], best_match[2])

        # ç¡®ä¿æ‰‹åŠ¨ç‚¹å‡»çš„å››ä¸ªç‚¹è¢«åŒ¹é…
        # æ‰¾åˆ°å¯¹åº”çš„3Dç‚¹ç´¢å¼• (0, 0), (610, 0), (606, 472), (4, 472)
        manual_point_indices = [0, 1, 2, 3]  # å¯¹åº” merged_3d_points ä¸­çš„ç´¢å¼•

        for i, corner in enumerate(self.manual_corners):
            if i < len(manual_point_indices):
                idx = manual_point_indices[i]
                point_3d = self.merged_3d_points[idx]
                label = self.merged_point_labels[idx]
                matched[idx] = (corner, point_3d, label)

        return matched

    def calibrate_extrinsic_parameters(self, matched_corners):
        """ä½¿ç”¨åŒ¹é…çš„è§’ç‚¹è®¡ç®—å¤–å‚çŸ©é˜µ"""
        if not matched_corners:
            return False

        # æå–åŒ¹é…çš„2Då’Œ3Dç‚¹
        points_2d = []
        points_3d = []

        for idx, (corner, point_3d, _) in matched_corners.items():
            points_2d.append(corner)
            points_3d.append(point_3d)

        points_2d = np.array(points_2d, dtype=np.float32)
        points_3d = np.array(points_3d, dtype=np.float32)

        # è‡³å°‘éœ€è¦6ä¸ªç‚¹
        if len(points_2d) < 6:
            print("Error: Need at least 6 matched points for reliable calibration")
            return False

        # è®¡ç®—PnPè§£
        success, rotation_vector, translation_vector = cv2.solvePnP(
            points_3d, points_2d, self.camera_matrix, self.dist_coeffs)

        if not success:
            print("Error: Failed to solve PnP")
            return False

        # ä¿å­˜å¤–å‚ç»“æœ
        self.rotation_vector = rotation_vector
        self.translation_vector = translation_vector

        # åˆ›å»ºæŠ•å½±çŸ©é˜µ
        rotation_matrix, _ = cv2.Rodrigues(rotation_vector)
        projection_matrix = np.hstack((rotation_matrix, translation_vector))
        self.projection_matrix = np.dot(self.camera_matrix, projection_matrix)

        return True

    def draw_court_lines(self, image):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶åœºåœ°çº¿ï¼ˆ4cmå®½ï¼‰"""
        if not hasattr(self, 'rotation_vector') or not hasattr(self, 'translation_vector'):
            return image

        result = image.copy()

        # å®šä¹‰åœºåœ°çº¿æ®µå¯¹ï¼ˆæ¯å¯¹çº¿æ®µå®šä¹‰ä¸€æ¡4cmå®½çš„çº¿ï¼‰
        court_line_pairs = [
            # åº•çº¿å¯¹
            (([0, 0, 0], [610, 0, 0]), ([4, 4, 0], [606, 4, 0])),

            # å·¦åŒæ‰“è¾¹çº¿
            (([0, 0, 0], [0, 472, 0]), ([4, 4, 0], [4, 472, 0])),

            # å³åŒæ‰“è¾¹çº¿
            (([610, 0, 0], [610, 472, 0]), ([606, 4, 0], [606, 472, 0])),

            # å·¦å•æ‰“è¾¹çº¿
            (([46, 4, 0], [46, 472, 0]), ([50, 4, 0], [50, 472, 0])),

            # å³å•æ‰“è¾¹çº¿
            (([560, 4, 0], [560, 472, 0]), ([564, 4, 0], [564, 472, 0])),

            # ä¸­çº¿
            (([303, 4, 0], [303, 472, 0]), ([307, 4, 0], [307, 472, 0])),

            # åå‘çƒçº¿
            (([4, 76, 0], [606, 76, 0]), ([4, 80, 0], [606, 80, 0])),

            # å‰å‘çƒçº¿
            (([4, 468, 0], [606, 468, 0]), ([4, 472, 0], [606, 472, 0])),

            # é¡¶çº¿å¯¹
            (([0, 472, 0], [610, 472, 0]), ([4, 472, 0], [606, 472, 0])),
        ]

        # ç»˜åˆ¶æ¯æ¡çº¿
        for (start1_3d, end1_3d), (start2_3d, end2_3d) in court_line_pairs:
            # æŠ•å½±ç¬¬ä¸€æ¡çº¿çš„ç«¯ç‚¹
            start1_3d = np.array([start1_3d], dtype=np.float32).reshape(1, 3)
            end1_3d = np.array([end1_3d], dtype=np.float32).reshape(1, 3)

            start1_2d, _ = cv2.projectPoints(start1_3d, self.rotation_vector,
                                             self.translation_vector, self.camera_matrix, self.dist_coeffs)
            end1_2d, _ = cv2.projectPoints(end1_3d, self.rotation_vector,
                                           self.translation_vector, self.camera_matrix, self.dist_coeffs)

            # æŠ•å½±ç¬¬äºŒæ¡çº¿çš„ç«¯ç‚¹
            start2_3d = np.array([start2_3d], dtype=np.float32).reshape(1, 3)
            end2_3d = np.array([end2_3d], dtype=np.float32).reshape(1, 3)

            start2_2d, _ = cv2.projectPoints(start2_3d, self.rotation_vector,
                                             self.translation_vector, self.camera_matrix, self.dist_coeffs)
            end2_2d, _ = cv2.projectPoints(end2_3d, self.rotation_vector,
                                           self.translation_vector, self.camera_matrix, self.dist_coeffs)

            # è½¬æ¢ä¸ºæ•´æ•°åæ ‡
            start1 = (int(start1_2d[0][0][0]), int(start1_2d[0][0][1]))
            end1 = (int(end1_2d[0][0][0]), int(end1_2d[0][0][1]))
            start2 = (int(start2_2d[0][0][0]), int(start2_2d[0][0][1]))
            end2 = (int(end2_2d[0][0][0]), int(end2_2d[0][0][1]))

            # åˆ›å»ºå¡«å……çš„å¤šè¾¹å½¢æ¥è¡¨ç¤ºå®½çº¿
            pts = np.array([start1, end1, end2, start2], np.int32)
            pts = pts.reshape((-1, 1, 2))
            cv2.fillPoly(result, [pts], (0, 255, 0))

            # åŒæ—¶åœ¨è¾¹ç¼˜ç”»çº¿ä»¥æé«˜æ¸…æ™°åº¦
            cv2.line(result, start1, end1, (255, 255, 255), 1)
            cv2.line(result, start2, end2, (255, 255, 255), 1)
            cv2.line(result, start1, start2, (255, 255, 255), 1)
            cv2.line(result, end1, end2, (255, 255, 255), 1)

        return result

    def save_calibration_results(self, output_dir, processed_frames, final_frame):
        """ä¿å­˜æ ‡å®šç»“æœ"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ä¿å­˜æœ€ç»ˆç»“æœå¸§
        cv2.imwrite(os.path.join(output_dir, "calibration_result.jpg"), final_frame)

        # ä¿å­˜æ ‡å®šå‚æ•°
        if hasattr(self, 'rotation_vector') and hasattr(self, 'translation_vector'):
            fs = cv2.FileStorage(os.path.join(output_dir, "extrinsic_parameters.yaml"), cv2.FILE_STORAGE_WRITE)
            fs.write("camera_matrix", self.camera_matrix)
            fs.write("distortion_coefficients", self.dist_coeffs)
            fs.write("rotation_vector", self.rotation_vector)
            fs.write("translation_vector", self.translation_vector)
            fs.write("projection_matrix", self.projection_matrix)

            # ä¿å­˜æ—¶é—´æˆ³
            fs.write("calibration_date", time.strftime("%Y-%m-%d_%H-%M-%S"))
            fs.write("image_width", self.image_width)
            fs.write("image_height", self.image_height)

            fs.release()

            print(f"Calibration results saved to {output_dir}")
            return os.path.join(output_dir, "extrinsic_parameters.yaml")

        return None

    def calibrate_from_camera(self, camera_manager, num_frames=20, preview_time=5.0):
        """
        ä½¿ç”¨æ‘„åƒå¤´å®æ—¶ç”»é¢è¿›è¡Œæ ‡å®š
        
        å‚æ•°:
            camera_manager: NetworkCameraManagerå®ä¾‹
            num_frames: ç”¨äºæ£€æµ‹çš„å¸§æ•°
            preview_time: é¢„è§ˆæ—¶é—´(ç§’)
        
        è¿”å›:
            bool: æ ‡å®šæ˜¯å¦æˆåŠŸ
        """
        print("ğŸ¯ Starting camera live feed calibration...")

        # ç­‰å¾…æ‘„åƒå¤´åˆå§‹åŒ–å®Œæˆ
        print("â³ Waiting for camera initialization...")
        max_wait_time = 10  # æœ€å¤§ç­‰å¾…10ç§’
        wait_start = time.time()

        while time.time() - wait_start < max_wait_time:
            if camera_manager.isOpened():
                print("âœ… Camera is ready")
                break
            time.sleep(0.5)
        else:
            print("âŒ Error: Camera manager is not properly initialized")
            return False

        # æ£€æŸ¥æ‘„åƒå¤´æ˜¯å¦æ­£å¸¸å·¥ä½œ
        if not camera_manager.isOpened():
            print("Error: Camera manager is not properly initialized")
            return False
        
        # æ˜¾ç¤ºåˆå§‹æŒ‡å¯¼
        cv2.namedWindow("Camera Calibration Guide", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("Camera Calibration Guide", 800, 600)

        guide_img = np.zeros((600, 800, 3), dtype=np.uint8)
        guide_texts = [
            "Badminton Court Camera Calibration",
            "",
            "1. Camera live feed will be shown for selecting court corners",
            "2. Right-click near each corner to select precise position",
            "3. Follow order: Bottom Left, Bottom Right, Top Right, Top Left",
            "4. After corner selection, automatic court point detection will run",
            "",
            f"Preview time: {preview_time} seconds",
            f"Frames for detection: {num_frames}",
            "",
            "Press SPACE to continue..."
        ]

        for i, text in enumerate(guide_texts):
            cv2.putText(guide_img, text, (50, 80 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        cv2.imshow("Camera Calibration Guide", guide_img)
        while True:
            key = cv2.waitKey(1) & 0xFF
            if key == ord(' '):
                break
            elif key == 27:  # ESC to cancel
                cv2.destroyWindow("Camera Calibration Guide")
                return False
        cv2.destroyWindow("Camera Calibration Guide")

        # è·å–å®æ—¶ç”»é¢è¿›è¡Œè§’ç‚¹é€‰æ‹©
        print("ğŸ“¹ Capturing live frame for corner selection...")
        
        # ç­‰å¾…ç¨³å®šçš„ç”»é¢
        stable_frame = self._get_stable_camera_frame(camera_manager, preview_time)
        if stable_frame is None:
            print("Error: Failed to capture stable frame from camera")
            return False

        # è®©ç”¨æˆ·é€‰æ‹©åˆå§‹åœºåœ°è¾¹ç•Œ
        masked_frame, mask, boundary_points = self.select_initial_court_boundary(stable_frame)

        # ä»æ‘„åƒå¤´æ•è·å’Œå¤„ç†å¤šå¸§è¿›è¡Œè§’ç‚¹æ£€æµ‹
        processed_frames, detected_corners = self._capture_and_process_camera_frames(
            camera_manager, mask, num_frames
        )

        if not processed_frames or not detected_corners:
            print("Error: Failed to process camera frames or detect corners")
            return False

        # åŒ¹é…è§’ç‚¹ä¸3Dåæ ‡
        matched_corners = self.match_corners_to_3d_points(detected_corners, boundary_points)

        # è®¡ç®—å¤–å‚
        if not self.calibrate_extrinsic_parameters(matched_corners):
            print("Error: Failed to calculate extrinsic parameters")
            return False

        # ç»˜åˆ¶åœºåœ°çº¿
        result_frame = self.draw_court_lines(processed_frames[0])

        # ä¿å­˜ç»“æœåˆ°é»˜è®¤ç›®å½•
        output_dir = "./calibration_results"
        output_file = self.save_calibration_results(output_dir, processed_frames, result_frame)

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        cv2.namedWindow("Camera Calibration Result", cv2.WINDOW_NORMAL)
        cv2.imshow("Camera Calibration Result", result_frame)
        
        print("âœ… Camera calibration completed successfully!")
        print("Press any key to close the result window...")
        cv2.waitKey(0)
        cv2.destroyAllWindows()

        return True

    def _get_stable_camera_frame(self, camera_manager, preview_time):
        """è·å–ç¨³å®šçš„æ‘„åƒå¤´ç”»é¢ç”¨äºè§’ç‚¹é€‰æ‹©"""
        print(f"ğŸ“º Showing camera preview for {preview_time} seconds...")
        print("   Please position the camera to clearly see the badminton court")
        
        preview_window = "Camera Preview - Position Camera"
        cv2.namedWindow(preview_window, cv2.WINDOW_NORMAL)
        
        start_time = time.time()
        frame_count = 0
        last_frame = None
        
        while time.time() - start_time < preview_time:
            # è¯»å–å½“å‰å¸§
            (ret1, ret2), (frame1, frame2) = camera_manager.read()
            
            if ret1 and frame1 is not None:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‘„åƒå¤´çš„ç”»é¢è¿›è¡Œæ ‡å®š
                display_frame = frame1.copy()
                
                # æ·»åŠ é¢„è§ˆä¿¡æ¯
                remaining_time = preview_time - (time.time() - start_time)
                cv2.putText(display_frame, f"Preview: {remaining_time:.1f}s remaining", 
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                cv2.putText(display_frame, "Position camera for clear court view", 
                           (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                cv2.putText(display_frame, "Press SPACE to use current frame", 
                           (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                cv2.imshow(preview_window, display_frame)
                last_frame = frame1.copy()
                frame_count += 1
            
            # æ£€æŸ¥ç”¨æˆ·è¾“å…¥
            key = cv2.waitKey(1) & 0xFF
            if key == ord(' '):  # ç©ºæ ¼é”®æå‰ç»“æŸé¢„è§ˆ
                break
            elif key == 27:  # ESCé”®å–æ¶ˆ
                cv2.destroyWindow(preview_window)
                return None
            
            time.sleep(0.033)  # çº¦30fps
        
        cv2.destroyWindow(preview_window)
        
        if last_frame is None:
            print("Error: No frame captured during preview")
            return None
            
        print(f"âœ… Captured {frame_count} preview frames, using latest frame for calibration")
        return last_frame

    def _capture_and_process_camera_frames(self, camera_manager, mask, num_frames):
        """ä»æ‘„åƒå¤´æ•è·å’Œå¤„ç†å¤šå¸§è¿›è¡Œè§’ç‚¹æ£€æµ‹"""
        print(f"ğŸ“¸ Capturing {num_frames} frames from camera for corner detection...")
        
        processed_frames = []
        all_detected_corners = []
        
        # å±•ç¤ºå¤„ç†è¿›åº¦çš„çª—å£
        progress_window = "Processing Camera Frames"
        cv2.namedWindow(progress_window, cv2.WINDOW_NORMAL)
        
        for i in range(num_frames):
            # è¯»å–å½“å‰å¸§
            (ret1, ret2), (frame1, frame2) = camera_manager.read()
            
            if not ret1 or frame1 is None:
                print(f"Warning: Failed to read frame {i+1}, skipping...")
                continue
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‘„åƒå¤´çš„ç”»é¢
            frame = frame1.copy()
            
            # åº”ç”¨ç›¸åŒçš„æ©ç å’Œé©¬èµ›å…‹å¤„ç†
            masked_frame = frame.copy()
            masked_frame = cv2.bitwise_and(masked_frame, masked_frame, mask=mask)

            # åˆ›å»ºé©¬èµ›å…‹æ•ˆæœç”¨äºé®ç½©å¤–åŒºåŸŸ
            mosaic_frame = frame.copy()
            mosaic_block_size = 20
            h, w = frame.shape[:2]

            for y in range(0, h, mosaic_block_size):
                for x in range(0, w, mosaic_block_size):
                    if y + mosaic_block_size <= h and x + mosaic_block_size <= w:
                        roi = frame[y:y + mosaic_block_size, x:x + mosaic_block_size]
                        color = roi.mean(axis=(0, 1))
                        mosaic_frame[y:y + mosaic_block_size, x:x + mosaic_block_size] = color

            outside_mask = cv2.bitwise_not(mask)
            mosaic_outside = cv2.bitwise_and(mosaic_frame, mosaic_frame, mask=outside_mask)
            final_frame = cv2.add(masked_frame, mosaic_outside)

            # ä½¿ç”¨YOLOv8æ£€æµ‹è§’ç‚¹
            corners = self.detect_court_corners_yolov8(final_frame)
            all_detected_corners.extend(corners)

            # ä¿å­˜å¤„ç†åçš„å¸§
            processed_frames.append(final_frame)

            # æ˜¾ç¤ºå¤„ç†è¿›åº¦
            progress_img = final_frame.copy()
            # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºæ£€æµ‹åˆ°çš„è§’ç‚¹
            for pt in corners:
                cv2.circle(progress_img, pt, 4, (0, 0, 255), -1)

            # æ·»åŠ è¿›åº¦æ–‡æœ¬
            progress_text = f"Processing camera frame {i + 1}/{num_frames}"
            text_size, _ = cv2.getTextSize(progress_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)
            cv2.rectangle(progress_img, (45, 35), (45 + text_size[0] + 10, 55 + text_size[1]), (0, 0, 0), -1)
            cv2.putText(progress_img, progress_text,
                        (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.imshow(progress_window, progress_img)
            cv2.waitKey(10)  # çŸ­æš‚æ˜¾ç¤º
            
            # æ·»åŠ å°å»¶è¿Ÿä»¥ç¡®ä¿å¸§ä¹‹é—´æœ‰å˜åŒ–
            time.sleep(0.1)

        cv2.destroyWindow(progress_window)

        # ä½¿ç”¨èšç±»é˜ˆå€¼åˆå¹¶è§’ç‚¹
        consolidated_corners = self.consolidate_corner_points(all_detected_corners, threshold=30)
        
        print(f"âœ… Processed {len(processed_frames)} camera frames")
        print(f"   Total detected corners: {len(all_detected_corners)}")
        print(f"   Consolidated corners: {len(consolidated_corners)}")

        return processed_frames, consolidated_corners

    def calibrate_from_video(self, video_path, output_dir="./calibration_results"):
        """ä»è§†é¢‘è¿›è¡Œå¤–å‚æ ‡å®šçš„ä¸»å‡½æ•°"""
        # æ˜¾ç¤ºåˆå§‹æŒ‡å¯¼
        cv2.namedWindow("Calibration Guide", cv2.WINDOW_NORMAL)
        cv2.resizeWindow("Calibration Guide", 800, 600)

        guide_img = np.zeros((600, 800, 3), dtype=np.uint8)
        guide_texts = [
            "Badminton Court Calibration",
            "",
            "1. First frame will be shown for selecting court corners",
            "2. Right-click near each corner to select precise position",
            "3. Follow order: Bottom Left, Bottom Right, Top Right, Top Left",
            "4. After corner selection, automatic court point detection will run",
            "",
            "Press SPACE to continue..."
        ]

        for i, text in enumerate(guide_texts):
            cv2.putText(guide_img, text, (50, 100 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        cv2.imshow("Calibration Guide", guide_img)
        while True:
            key = cv2.waitKey(1) & 0xFF
            if key == ord(' '):
                break
        cv2.destroyWindow("Calibration Guide")

        # ç›´æ¥è°ƒç”¨capture_and_process_framesè·å–å¤„ç†å¸§å’Œæ£€æµ‹åˆ°çš„è§’ç‚¹
        processed_frames, detected_corners, boundary_points = self.capture_and_process_frames(video_path, num_frames=20)

        if not processed_frames or not detected_corners:
            print("Error: Failed to process frames or detect corners")
            return False

        # åŒ¹é…è§’ç‚¹ä¸3Dåæ ‡
        matched_corners = self.match_corners_to_3d_points(detected_corners, boundary_points)

        # ç›´æ¥ä½¿ç”¨åŒ¹é…çš„è§’ç‚¹è®¡ç®—å¤–å‚ï¼Œè·³è¿‡äº¤äº’å¼éªŒè¯
        if not self.calibrate_extrinsic_parameters(matched_corners):
            print("Error: Failed to calculate extrinsic parameters")
            return False

        # ç»˜åˆ¶åœºåœ°çº¿
        result_frame = self.draw_court_lines(processed_frames[0])

        # ä¿å­˜ç»“æœ
        output_file = self.save_calibration_results(output_dir, processed_frames, result_frame)

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        cv2.namedWindow("Calibration Result", cv2.WINDOW_NORMAL)
        cv2.imshow("Calibration Result", result_frame)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

        return output_file


def calibrate_cameras(video_path1, video_path2, output_dir):
    """å¯¹ä¸¤ä¸ªæ‘„åƒæœºè¿›è¡Œæ ‡å®š"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # ç›¸æœº1æ ‡å®š
    print("Calibrating Camera 1...")
    calibrator1 = BadmintonCalibrator(config.camera_params_file_1, config.yolo_court_model)
    extrinsic_file1 = calibrator1.calibrate_from_video(video_path1, os.path.join(output_dir, "camera1"))

    # ç›¸æœº2æ ‡å®š
    print("Calibrating Camera 2...")
    calibrator2 = BadmintonCalibrator(config.camera_params_file_2, config.yolo_court_model)
    extrinsic_file2 = calibrator2.calibrate_from_video(video_path2, os.path.join(output_dir, "camera2"))

    # è¿”å›æ ‡å®šæ–‡ä»¶è·¯å¾„
    return extrinsic_file1, extrinsic_file2


def calibrate_cameras_from_live_feed(camera_manager, output_dir, num_frames=20, preview_time=5.0):
    """å¯¹ç½‘ç»œæ‘„åƒå¤´è¿›è¡Œå®æ—¶æ ‡å®š"""
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦ä¸ºåŒæ‘„åƒå¤´æ¨¡å¼
    if camera_manager.camera_url2:
        print("ğŸ¥ Dual camera live feed calibration...")
        
        # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„å•æ‘„åƒå¤´ç®¡ç†å™¨è¿›è¡Œåˆ†åˆ«æ ‡å®š
        from network_camera import NetworkCameraManager
        
        # ç›¸æœº1æ ‡å®š
        print("ğŸ“¹ Calibrating Camera 1 from live feed...")
        camera_manager1 = NetworkCameraManager(camera_manager.camera_url1, None, camera_manager.timestamp_header)
        camera_manager1.start()
        time.sleep(2)  # ç­‰å¾…æµç¨³å®š
        
        calibrator1 = BadmintonCalibrator(config.camera_params_file_1, config.yolo_court_model)
        success1 = calibrator1.calibrate_from_camera(camera_manager1, num_frames, preview_time)
        
        if success1:
            extrinsic_file1 = os.path.join(output_dir, "camera1", "extrinsic_parameters.yaml")
        else:
            camera_manager1.stop()
            return None, None
        
        camera_manager1.stop()
        
        # ç›¸æœº2æ ‡å®š
        print("ğŸ“¹ Calibrating Camera 2 from live feed...")
        camera_manager2 = NetworkCameraManager(camera_manager.camera_url2, None, camera_manager.timestamp_header)
        camera_manager2.start()
        time.sleep(2)  # ç­‰å¾…æµç¨³å®š
        
        calibrator2 = BadmintonCalibrator(config.camera_params_file_2, config.yolo_court_model)
        success2 = calibrator2.calibrate_from_camera(camera_manager2, num_frames, preview_time)
        
        if success2:
            extrinsic_file2 = os.path.join(output_dir, "camera2", "extrinsic_parameters.yaml")
        else:
            camera_manager2.stop()
            return extrinsic_file1, None
            
        camera_manager2.stop()
        
        return extrinsic_file1, extrinsic_file2
        
    else:
        print("ğŸ¥ Single camera live feed calibration...")
        
        # å•æ‘„åƒå¤´æ ‡å®š - ä½¿ç”¨ç›¸æœº1çš„å‚æ•°
        calibrator = BadmintonCalibrator(config.camera_params_file_1, config.yolo_court_model)
        success = calibrator.calibrate_from_camera(camera_manager, num_frames, preview_time)
        
        if success:
            extrinsic_file = os.path.join(output_dir, "camera1", "extrinsic_parameters.yaml")
            return extrinsic_file, extrinsic_file  # è¿”å›ç›¸åŒæ–‡ä»¶ç”¨äºå•æ‘„åƒå¤´æ¨¡å¼
        else:
            return None, None